
# 🧠 LLM from Scratch — Based on Sebastian Raschka's Work

This repository contains a personal implementation of a GPT-style **Large Language Model (LLM)** from scratch, inspired by the amazing work of [Sebastian Raschka](https://github.com/rasbt) and his book *[Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)*.

## 📚 About the Project

The goal of this project is to gain a deep, hands-on understanding of how transformer-based LLMs like GPT are built, trained, and fine-tuned — without relying on high-level libraries like HuggingFace.

### 💡 Key Highlights:
- Complete from-scratch implementation of a GPT-style architecture
- Custom tokenizer using Byte Pair Encoding (BPE)
- Training pipeline with attention mask, positional encoding, and layer normalization
- Autoregressive text generation
- Fine-tuning on domain-specific datasets (optional)

---

## 🏗️ Project Structure






